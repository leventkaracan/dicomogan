project_title: VideoManipulation

model:
  base_learning_rate: 2.0e-6
  target: dicomogan.models.tmp_dicomoclip.DiCoMOGANCLIP
  project : "video-manipulation"

  params:
    # tgt_text : 'Yellow Dresses Made from polyester with V-neckline. The dress is all over ribbed and has front buttons and is short sleeves. The dress is regular fit'


    
    delta_inversion_weight : 0.1
    l2_latent_eps : 0.02

    lambda_vgg: 10.0
    lambda_G: 1
    lambda_bvae: 1
    rec_loss_lambda : 1.0
    l2_latent_lambda : 1.0
    clip_loss_lambda : 5.0
    consistency_lambda : 0.5
    lambda_dim_loss : 0.01
    lambda_orth_loss : 0.2
    alpha : 0.8
    
    vae_cond_dim : 24 # must match text encoder latent_dim
    beta: 8

    video_ecnoder_config:
      target: dicomogan.new_modules.EncoderVideo_LatentODE
      params:
        img_size : [128,96] # must match video_decoder
        static_latent_dim : 64
        dynamic_latent_dim : 64 # must match ODE_func_config latent_dim
        hid_channels : 32 
        kernel_size : 4
        hidden_dim : 256
        netE_num_downsampling_sp : 3
        netE_num_downsampling_gl : 1
        spatial_code_ch : 256
        global_code_ch : 512 
    
    video_decoder_config:
      target: dicomogan.modules.Decoder
      params:
        img_size : [3,128,96] # must match video_encoder
        latent_dim : 128 # should match static_latent_dim + dynamic_latent_dim

    clip_projection_config:
      target: dicomogan.new_modules.LinearSubspace
      params:
        input_size : 512
        out_dim : 64

    stylegan_gen_config:
      target: dicomogan.models.stylegan.StyleGAN2Generator
      params:
        pkl_file: /kuacc/users/abond19/VideoEditing/dicomogan/StyleGAN_Human/ckpts/stylegan_human_v2_1024.pkl
    
    dim_loss_config:
      target: dicomogan.losses.dim_loss.DIMLoss
      params:
        n_units : 512
        embed1_dim : 64
        embed2_dim : 64 

    style_mapper_config:
      target: dicomogan.models.mapper.TripleAttributeMapper
      params:
        predict_delta: True
        use_coarse_mapper: [True, True, False] # Since the coarse mapper only handles pose, we don't want to do any modulation
        use_medium_mapper: [True, True, False]
        use_fine_mapper: [True, True, False] 
        coarse_cut_flag: False 
        medium_cut_flag: False
        fine_cut_flag: False
        mod_shape : [[1, 4, 1], [1, 4, 1], [1, 10, 1]]
        attr_vec_dims: [64, 64, 0] # much match W_cont + CLIP embedding


    
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 2 # MUST Match batch size in dataset
    num_workers: 10
    train:
      target: dicomogan.data.new_video.VideoDataFashion # Changed from data.video.VideoDataFashion to data.new_video.VideoDataFashion
      params:
        video_list: data/fashion/fashion_train_videos.txt
        img_root: /scratch/users/abond19/datasets/aligned_fashion_dataset
        inverted_img_root : /scratch/users/abond19/datasets/inverted_fashion_dataset
        inversion_root: /scratch/users/abond19/datasets/w+_fashion_dataset/fashion/PTI
        skip_frames : 5
        n_sampled_frames : 6
        irregular_sampling : True
        batch_size : 2 #MUST match batch size 
        # crop : [512, 384]
        size : [256, 192]
        # attribute : "Color"
        # attribute_stats : 'data/fashion/attributes_stats.yaml'

    validation:
      target: dicomogan.data.new_video.VideoDataFashion # Changed from data.video.VideoDataFashion to data.new_video.VideoDataFashion
      params:
        video_list: data/fashion/fashion_test_videos.txt
        img_root: /scratch/users/abond19/datasets/aligned_fashion_dataset
        inverted_img_root : /scratch/users/abond19/datasets/inverted_fashion_dataset
        inversion_root: /scratch/users/abond19/datasets/w+_fashion_dataset/fashion/PTI
        skip_frames : 5
        n_sampled_frames : 6
        irregular_sampling : False
        batch_size : 2 #MUST match batch size 
        # crop : [512, 384]
        size : [256, 192]
        # attribute : "Color"
        # attribute_stats : 'data/fashion/attributes_stats.yaml'


lightning:
  trainer:
    limit_val_batches : 2
    replace_sampler_ddp : False
    # precision : 32

