{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73bd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea669fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/kuacc/users/mali18/dicomogan/logs/_510_face_attention2022-10-30T03-54-36'\n",
    "img_root = '/scratch/users/abond19/datasets/RAVDESS/Aligned'\n",
    "inverted_img_root = '/scratch/users/abond19/datasets/inverted_face_dataset'\n",
    "inversion_root = '/kuacc/users/abond19/datasets/w+_face_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e53366ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/kuacc/users/mali18/dicomogan/logs/tmp_log/38_k___541_fashion_selfattention2022-10-30T03-54-43/__541_fashion_selfattention2022-10-30T03-54-43'\n",
    "img_root = '/kuacc/users/abond19/datasets/aligned_fashion_dataset'\n",
    "inverted_img_root =  '/kuacc/users/abond19/datasets/inverted_fashion_dataset'\n",
    "inversion_root =  '/kuacc/users/abond19/datasets/w+_fashion_dataset/fashion/PTI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5250db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81fc09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from /kuacc/users/mali18/dicomogan/logs/_510_face_attention2022-10-30T03-54-36/VideoManipulation/_510_face_attention2022-10-30T03-54-36/checkpoints/epoch=116-step=32291.ckpt\n"
     ]
    }
   ],
   "source": [
    "from experiments_utils import *\n",
    "model = load_model_from_dir(model_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e36b921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_start = 5\n",
    "n_frames = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1710ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "import os\n",
    "import torch \n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "IMG_EXTENSIONS = ['.png', '.PNG']\n",
    "TXT_EXTENSIONS = ['.txt']\n",
    "\n",
    "crop = None\n",
    "size = None\n",
    "trans_list = []\n",
    "if crop is not None:\n",
    "    trans_list.append(transforms.CenterCrop(tuple(crop)))\n",
    "if size is not None:\n",
    "    trans_list.append(transforms.Resize(tuple(size)))\n",
    "trans_list.append(transforms.ToTensor())\n",
    "img_transform=transforms.Compose(trans_list)\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def is_text_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in TXT_EXTENSIONS)\n",
    "\n",
    "def get_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return img\n",
    "\n",
    "def get_inversion(inversion_path):\n",
    "    w_vector = torch.load(inversion_path, map_location='cpu')\n",
    "    if len(w_vector.shape) == 2:\n",
    "        w_vector = w_vector.unsqueeze(0)\n",
    "    assert (w_vector.shape == (1, 18, 512)), \"Inverted vector has incorrect shape\"\n",
    "    return w_vector\n",
    "\n",
    "def load_video(vid_path):\n",
    "    images, inversions, sampleT, inversion_imgs = [], [], [], [] \n",
    "    fname = vid_path\n",
    "    \n",
    "    lst_vid = sorted(os.listdir(os.path.join(img_root, fname)))\n",
    "    def correct(f):\n",
    "        if is_image_file(f):\n",
    "            return int(f.split('.')[0])\n",
    "        else:\n",
    "            return f\n",
    "    lst_vid = list(map(correct, lst_vid))[:-1]\n",
    "#     print(lst_vid)\n",
    "    inds = np.argsort(lst_vid)\n",
    "#     print(inds)\n",
    "    inds = inds[n_start:n_start+n_frames]\n",
    "    for f in np.array(sorted(os.listdir(os.path.join(img_root, fname))))[inds]:\n",
    "        if is_image_file(f):\n",
    "#             print(f)\n",
    "            imname = f[:-4]\n",
    "            images.append(img_transform(get_image(os.path.join(img_root, fname, f))))\n",
    "            inversion_imgs.append(img_transform(get_image(os.path.join(inverted_img_root, fname, f))))\n",
    "            inversions.append(get_inversion(os.path.join(os.path.join(inversion_root, fname, imname + \".pt\"))))\n",
    "            sampleT.append(int(imname))\n",
    "    \n",
    "    return torch.stack(images).to(device), torch.cat(inversions, 0).to(device), torch.Tensor(sampleT).to(device), torch.stack(inversion_imgs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a17eeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torchvision\n",
    "def save_gif(video, range, save_path):\n",
    "    # Assuming that the current shape is T * B x C x H x W\n",
    "    with imageio.get_writer(save_path, mode='I') as writer:\n",
    "       for b_frames in video:\n",
    "            # b_frames B x C x H x W\n",
    "            frame = torchvision.utils.make_grid(b_frames,\n",
    "                            nrow=b_frames.shape[0],\n",
    "                            normalize=True,\n",
    "                            range=range).detach().cpu().numpy()\n",
    "            frame = (np.transpose(frame, (1, 2, 0)) * 255).astype(np.uint8)\n",
    "            writer.append_data(frame)\n",
    "\n",
    "#     wandb.log({name: wandb.Video(filename, fps=2, format=\"gif\")})\n",
    "#     os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "244fd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    '8c110571',\n",
    "    '2c111960',\n",
    "    '8c110147'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10e99a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    'Actor_03_114',\n",
    "    'Actor_04_97',\n",
    "    'Actor_05_99'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba594e",
   "metadata": {},
   "source": [
    "# Reconstruct Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68af374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 14., 15., 16., 17., 18., 19.,  2., 20., 21.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sampleT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ccfc5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [00:10<00:21, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [00:20<00:10, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:30<00:00, 10.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch.nn as nn\n",
    "with torch.no_grad():\n",
    "    for video in tqdm(videos):\n",
    "        images, inversions, sampleT, inversion_imgs = load_video(video)\n",
    "        save_dir = os.path.join('applications_results', f\"{model_dir.split('/')[-1]}\", video)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # org\n",
    "        save_gif(images, (0, 1), f'{save_dir}/original.gif')\n",
    "        \n",
    "        # inversion\n",
    "        save_gif(inversion_imgs, (0, 1), f'{save_dir}/inversion.gif')\n",
    "        \n",
    "        tgt_desc = \"A picture of a woman\"\n",
    "        txt_feat = model.clip_encode_text([tgt_desc])  # - model.clip_encode_text([src_desc])\n",
    "        \n",
    "        ind = np.random.randint(len(sampleT))\n",
    "        frame_feat = model.clip_loss.encode_images(images[ind:ind+1])\n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions.mean(0, keepdims=True),\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Mean_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions.mean(0, keepdims=True))[0]).save(f'{save_dir}/mean_frame.png')\n",
    "        \n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[-1:],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Last_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[-1:])[0]).save(f'{save_dir}/last_frame.png')\n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[0:1],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/First_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[0:1])[0]).save(f'{save_dir}/first_frame.png')\n",
    "        \n",
    "        \n",
    "        mid = inversions.shape[0] // 2\n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[mid:mid+1],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Middle_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[mid:mid+1])[0]).save(f'{save_dir}/middle_frame.png')\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='L')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/Last_frame')\n",
    "\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='F')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/First_frame')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37050049",
   "metadata": {},
   "source": [
    "# Generate images for eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8c5ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fashion/fashion_train_videos.txt', 'r') as f:\n",
    "    train_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55f46200",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fashion/fashion_test_videos.txt', 'r') as f:\n",
    "     test_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c5926dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate(lst, save_dir, desc, bs=4):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(lst), bs)):\n",
    "            images, inversions = [], [],\n",
    "            for j in range(i, i+bs):\n",
    "                a, b, c, _ = load_video(lst[j])\n",
    "                images.append(a)\n",
    "                inversions.append(b)\n",
    "                sampleT = c\n",
    "            \n",
    "#             print(torch.stack(images, 0).shape, torch.stack(inversions, 0).shape)\n",
    "            edited_videos = forward_triple(model, torch.stack(images, 0), torch.stack(inversions, 0), sampleT, [desc] * bs)          \n",
    "            for video, video_name in zip(edited_videos, lst[i:i+bs]):\n",
    "                save_path = os.path.join(save_dir, video_name)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                for j, frame in enumerate(video):\n",
    "                    to_PIL(frame).save(os.path.join(save_path, f\"{j:06d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "163cc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3f823684",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c79c18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"A picture of a woman wearing a Women's Black Regular fit Blouse with a Round neckline and Long sleeves and a blue jean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6f708a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = np.array(train_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "201bd47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:35<00:00, 24.72s/it]\n"
     ]
    }
   ],
   "source": [
    "generate(test_videos[:200], test_save_dir, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a4573e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1024)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open('/kuacc/users/abond19/datasets/aligned_fashion_dataset/7c90376/00010.png').size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tokengan)",
   "language": "python",
   "name": "tokengan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
