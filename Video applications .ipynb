{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73bd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea669fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/kuacc/users/mali18/dicomogan/logs/_510_face_splice2022-11-01T02-55-15'\n",
    "img_root = '/userfiles/abond19/video_datasets/RAVDESS/RAVDESS/Aligned'\n",
    "inverted_img_root = '/kuacc/users/abond19/VideoEditing/RAVDESS_inversion_prev/image_inversions'\n",
    "inversion_root = '/kuacc/users/abond19/VideoEditing/RAVDESS_inversion_prev/w+_inversions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53366ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = '/kuacc/users/mali18/dicomogan/logs/tmp_log/38_k___541_fashion_selfattention2022-10-30T03-54-43/__541_fashion_selfattention2022-10-30T03-54-43'\n",
    "# img_root = '/kuacc/users/abond19/datasets/aligned_fashion_dataset'\n",
    "# inverted_img_root =  '/kuacc/users/abond19/datasets/inverted_fashion_dataset'\n",
    "# inversion_root =  '/kuacc/users/abond19/datasets/w+_fashion_dataset/fashion/PTI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5250db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a8e01f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2b6a53aaca30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81fc09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /scratch/users/mali18/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /scratch/users/mali18/.cache/torch_extensions/py38_cu113/fused/build.ninja...\n",
      "Building extension module fused...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused...\n",
      "Loading custom kernel...\n",
      "Using /scratch/users/mali18/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /scratch/users/mali18/.cache/torch_extensions/py38_cu113/upfirdn2d/build.ninja...\n",
      "Building extension module upfirdn2d...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module upfirdn2d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /kuacc/users/mali18/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from /kuacc/users/mali18/dicomogan/logs/_510_face_splice2022-11-01T02-55-15/VideoManipulation/_510_face_splice2022-11-01T02-55-15/checkpoints/epoch=194-step=53819.ckpt\n"
     ]
    }
   ],
   "source": [
    "from experiments_utils import *\n",
    "model = load_model_from_dir(model_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4567d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7afdb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new fine-tuned generator\n",
    "from dicomogan.models.stylegan import StyleGAN2Face\n",
    "finetune_gen = StyleGAN2Face(\"/kuacc/users/abond19/VideoEditing/encoder4editing/updated_stylegan.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b44fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "958be250",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stylegan_G = finetune_gen.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b6fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_start = 5\n",
    "n_frames = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1710ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "import os\n",
    "import torch \n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "IMG_EXTENSIONS = ['.png', '.PNG']\n",
    "TXT_EXTENSIONS = ['.txt']\n",
    "\n",
    "crop = None\n",
    "size = None\n",
    "trans_list = []\n",
    "if crop is not None:\n",
    "    trans_list.append(transforms.CenterCrop(tuple(crop)))\n",
    "if size is not None:\n",
    "    trans_list.append(transforms.Resize(tuple(size)))\n",
    "trans_list.append(transforms.ToTensor())\n",
    "img_transform=transforms.Compose(trans_list)\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def is_text_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in TXT_EXTENSIONS)\n",
    "\n",
    "def get_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return img\n",
    "\n",
    "def get_inversion(inversion_path):\n",
    "    w_vector = torch.load(inversion_path, map_location='cpu')\n",
    "    if len(w_vector.shape) == 2:\n",
    "        w_vector = w_vector.unsqueeze(0)\n",
    "    assert (w_vector.shape == (1, 18, 512)), \"Inverted vector has incorrect shape\"\n",
    "    return w_vector\n",
    "\n",
    "def load_video(vid_path):\n",
    "    images, inversions, sampleT, inversion_imgs = [], [], [], [] \n",
    "    fname = vid_path\n",
    "    \n",
    "    lst_vid = sorted(os.listdir(os.path.join(img_root, fname)))\n",
    "    def correct(f):\n",
    "        if is_image_file(f):\n",
    "            return int(f.split('.')[0])\n",
    "        else:\n",
    "            return f\n",
    "    lst_vid = list(map(correct, lst_vid))[:-1]\n",
    "#     print(lst_vid)\n",
    "    inds = np.argsort(lst_vid)\n",
    "#     print(inds)\n",
    "    inds = inds[n_start:n_start+n_frames]\n",
    "    for f in np.array(sorted(os.listdir(os.path.join(img_root, fname))))[inds]:\n",
    "        if is_image_file(f):\n",
    "#             print(f)\n",
    "            imname = f[:-4]\n",
    "            images.append(img_transform(get_image(os.path.join(img_root, fname, f))))\n",
    "            inversion_imgs.append(img_transform(get_image(os.path.join(inverted_img_root, fname, f))))\n",
    "            inversions.append(get_inversion(os.path.join(os.path.join(inversion_root, fname, imname + \".pt\"))))\n",
    "            sampleT.append(int(imname))\n",
    "    \n",
    "    with open(os.path.join(img_root, fname, 'updated_descriptions.txt')) as f:\n",
    "        description = f.readlines()[0]\n",
    "\n",
    "    return torch.stack(images).to(device), torch.cat(inversions, 0).to(device), torch.Tensor(sampleT).to(device), torch.stack(inversion_imgs).to(device), description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17eeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torchvision\n",
    "def save_gif(video, range, save_path):\n",
    "    # Assuming that the current shape is T * B x C x H x W\n",
    "    with imageio.get_writer(save_path, mode='I') as writer:\n",
    "       for b_frames in video:\n",
    "            # b_frames B x C x H x W\n",
    "            frame = torchvision.utils.make_grid(b_frames,\n",
    "                            nrow=b_frames.shape[0],\n",
    "                            normalize=True,\n",
    "                            range=range).detach().cpu().numpy()\n",
    "            frame = (np.transpose(frame, (1, 2, 0)) * 255).astype(np.uint8)\n",
    "            writer.append_data(frame)\n",
    "\n",
    "#     wandb.log({name: wandb.Video(filename, fps=2, format=\"gif\")})\n",
    "#     os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "244fd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    '8c110571',\n",
    "    '2c111960',\n",
    "    '8c110147'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9e3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    'Actor_03_114',\n",
    "    'Actor_04_97',\n",
    "    'Actor_05_99'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba594e",
   "metadata": {},
   "source": [
    "# Reconstruct Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73acd81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delta_inversion_weight = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccfc5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:14<00:00, 24.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch.nn as nn\n",
    "with torch.no_grad():\n",
    "    for video in tqdm(videos):\n",
    "        images, inversions, sampleT, inversion_imgs, src_desc = load_video(video)\n",
    "        save_dir = os.path.join('applications_results', f\"{model_dir.split('/')[-1]}\", video)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # org\n",
    "        save_gif(images, (0, 1), f'{save_dir}/original.gif')\n",
    "        \n",
    "        # inversion\n",
    "        save_gif(inversion_imgs, (0, 1), f'{save_dir}/inversion.gif')\n",
    "        \n",
    "        tgt_desc = \"A picture of a woman\"\n",
    "        txt_feat = model.clip_encode_text([src_desc]).cuda()  # - model.clip_encode_text([src_desc])\n",
    "        \n",
    "        ind = np.random.randint(len(sampleT))\n",
    "        frame_feat = model.clip_loss.encode_images(images[ind:ind+1])\n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions.mean(0, keepdims=True),\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Mean_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions.mean(0, keepdims=True))[0]).save(f'{save_dir}/mean_frame.png')\n",
    "        \n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[-1:],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Last_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[-1:])[0]).save(f'{save_dir}/last_frame.png')\n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[0:1],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/First_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[0:1])[0]).save(f'{save_dir}/first_frame.png')\n",
    "        \n",
    "        \n",
    "        mid = inversions.shape[0] // 2\n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[mid:mid+1],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Middle_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[mid:mid+1])[0]).save(f'{save_dir}/middle_frame.png')\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='L')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/Last_frame')\n",
    "\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='F')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/First_frame')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37050049",
   "metadata": {},
   "source": [
    "# Generate images for eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a59e5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = 'data/fashion/fashion_train_videos.txt'\n",
    "# test_path = 'data/fashion/fashion_test_videos.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "368d48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/kuacc/users/mali18/dicomogan/data/face/action_train.txt'\n",
    "test_path = '/kuacc/users/mali18/dicomogan/data/face/action_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a8c5ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, 'r') as f:\n",
    "    train_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55f46200",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_path, 'r') as f:\n",
    "     test_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c5926dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate(lst, save_dir, bs=2, save_size=(256,256)):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(lst), bs)):\n",
    "            images, inversions, descriptions = [], [], []\n",
    "            for j in range(i, i+bs):\n",
    "                a, b, c, _, desc = load_video(lst[j])\n",
    "                images.append(a)\n",
    "                inversions.append(b)\n",
    "                sampleT = c\n",
    "                descriptions.append(desc)\n",
    "            \n",
    "            images, inversions = torch.stack(images, 0), torch.stack(inversions, 0)\n",
    "            txt_feat = model.clip_encode_text(descriptions)  # - model.clip_encode_text([src_desc])\n",
    "\n",
    "            ind = np.random.randint(len(sampleT))\n",
    "            frame_feat = model.clip_loss.encode_images(images[:, ind])\n",
    "\n",
    "#             print(images.shape, sampleT.shape, inversions.mean(1, keepdims=False).shape, txt_feat.shape, frame_feat.shape)\n",
    "            edited_videos = model(images, \n",
    "                                  sampleT,\n",
    "                                  inversions.mean(1, keepdims=False),\n",
    "                                  txt_feat,\n",
    "                                  frame_feat)\n",
    "\n",
    "    # #             print(torch.stack(images, 0).shape, torch.stack(inversions, 0).shape)\n",
    "    #             edited_videos = model(torch.stack(images, 0), torch.stack(inversions, 0), sampleT, [desc] * bs)          \n",
    "            for video, video_name in zip(edited_videos, lst[i:i+bs]):\n",
    "                save_path = os.path.join(save_dir, video_name)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                for j, frame in enumerate(video):\n",
    "                    to_PIL(frame).resize(save_size).save(os.path.join(save_path, f\"{j:06d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "163cc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3f823684",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6f708a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = np.array(train_videos)\n",
    "test_videos = np.array(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9daa6c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:14<00:00,  5.54s/it]\n"
     ]
    }
   ],
   "source": [
    "generate(train_videos[:200], train_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "201bd47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of dicomogan.minigpt failed: Traceback (most recent call last):\n",
      "  File \"/kuacc/users/mali18/.conda/envs/styleode/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/kuacc/users/mali18/.conda/envs/styleode/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 480, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/kuacc/users/mali18/.conda/envs/styleode/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 377, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/kuacc/users/mali18/.conda/envs/styleode/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 345, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/kuacc/users/mali18/.conda/envs/styleode/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 299, in update_instances\n",
      "    refs = gc.get_referrers(old)\n",
      "KeyboardInterrupt\n",
      "]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(test_videos[:200], test_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a4573e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1024)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open('/kuacc/users/abond19/datasets/aligned_fashion_dataset/7c90376/00010.png').size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (styleode)",
   "language": "python",
   "name": "styleode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
