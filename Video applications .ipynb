{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73bd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea669fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/kuacc/users/mali18/dicomogan/logs/_9132_seq_modulation_gaugan22022-10-24T00-36-21'\n",
    "img_root = '/kuacc/users/abond19/datasets/aligned_fashion_dataset'\n",
    "inverted_img_root =  '/kuacc/users/abond19/datasets/inverted_fashion_dataset'\n",
    "inversion_root =  '/kuacc/users/abond19/datasets/w+_fashion_dataset/fashion/PTI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5250db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fc09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /scratch/users/mali18/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /scratch/users/mali18/.cache/torch_extensions/py37_cu113/fused/build.ninja...\n",
      "Building extension module fused...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused...\n",
      "Loading custom kernel...\n",
      "Using /scratch/users/mali18/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /scratch/users/mali18/.cache/torch_extensions/py37_cu113/upfirdn2d/build.ninja...\n",
      "Building extension module upfirdn2d...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module upfirdn2d...\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "Restored from /kuacc/users/mali18/dicomogan/logs/_9132_seq_modulation_gaugan22022-10-24T00-36-21/VideoManipulation/_9132_seq_modulation_gaugan22022-10-24T00-36-21/checkpoints/epoch=134-step=151740.ckpt\n"
     ]
    }
   ],
   "source": [
    "from experiments_utils import *\n",
    "model = load_model_from_dir(model_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1710ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "import os\n",
    "import torch \n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "IMG_EXTENSIONS = ['.png', '.PNG']\n",
    "TXT_EXTENSIONS = ['.txt']\n",
    "\n",
    "crop = None\n",
    "size = None\n",
    "trans_list = []\n",
    "if crop is not None:\n",
    "    trans_list.append(transforms.CenterCrop(tuple(crop)))\n",
    "if size is not None:\n",
    "    trans_list.append(transforms.Resize(tuple(size)))\n",
    "trans_list.append(transforms.ToTensor())\n",
    "img_transform=transforms.Compose(trans_list)\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def is_text_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in TXT_EXTENSIONS)\n",
    "\n",
    "def get_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return img\n",
    "\n",
    "def get_inversion(inversion_path):\n",
    "    w_vector = torch.load(inversion_path, map_location='cpu')\n",
    "    assert (w_vector.shape == (1, 18, 512)), \"Inverted vector has incorrect shape\"\n",
    "    return w_vector\n",
    "\n",
    "def load_video(vid_path):\n",
    "    images, inversions, sampleT, inversion_imgs = [], [], [], [] \n",
    "    fname = vid_path\n",
    "    for f in sorted(os.listdir(os.path.join(img_root, fname)))[5:15]:\n",
    "        if is_image_file(f):\n",
    "            imname = f[:-4]\n",
    "            images.append(img_transform(get_image(os.path.join(img_root, fname, f))))\n",
    "            inversion_imgs.append(img_transform(get_image(os.path.join(inverted_img_root, fname, f))))\n",
    "            inversions.append(get_inversion(os.path.join(os.path.join(inversion_root, fname, imname + \".pt\"))))\n",
    "            sampleT.append(int(imname))\n",
    "    \n",
    "    return torch.stack(images).to(device), torch.cat(inversions, 0).to(device), torch.Tensor(sampleT).to(device), torch.stack(inversion_imgs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17eeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torchvision\n",
    "def save_gif(video, range, save_path):\n",
    "    # Assuming that the current shape is T * B x C x H x W\n",
    "    with imageio.get_writer(save_path, mode='I') as writer:\n",
    "       for b_frames in video:\n",
    "            # b_frames B x C x H x W\n",
    "            frame = torchvision.utils.make_grid(b_frames,\n",
    "                            nrow=b_frames.shape[0],\n",
    "                            normalize=True,\n",
    "                            range=range).detach().cpu().numpy()\n",
    "            frame = (np.transpose(frame, (1, 2, 0)) * 255).astype(np.uint8)\n",
    "            writer.append_data(frame)\n",
    "\n",
    "#     wandb.log({name: wandb.Video(filename, fps=2, format=\"gif\")})\n",
    "#     os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244fd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    '8c110571',\n",
    "    '2c111960',\n",
    "    '8c110147'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba594e",
   "metadata": {},
   "source": [
    "# Reconstruct Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfc5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:35<00:00, 11.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch.nn as nn\n",
    "with torch.no_grad():\n",
    "    for video in tqdm(videos):\n",
    "        images, inversions, sampleT, inversion_imgs = load_video(video)\n",
    "        save_dir = os.path.join('applications_results', f\"{model_dir.split('/')[-1]}\", video)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # org\n",
    "        save_gif(images, (0, 1), f'{save_dir}/original.gif')\n",
    "        \n",
    "        # inversion\n",
    "        save_gif(inversion_imgs, (0, 1), f'{save_dir}/inversion.gif')\n",
    "        \n",
    "        tgt_desc = \"A picture of a woman wearing a Women's Multi Color Regular fit Body with a Overlap V-neckline and Long sleevesA picture of a woman wearing a Women's Multi Color Regular fit Body with a Overlap V-neckline and Long sleeves\"\n",
    "        src_desc = \"a photo of a woman wearing short sleeves t-shirt\"\n",
    "        txt_feat = model.clip_encode_text([tgt_desc])  # - model.clip_encode_text([src_desc])\n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions.mean(0, keepdims=True),\n",
    "                              txt_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Mean_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions.mean(0, keepdims=True))[0]).save(f'{save_dir}/mean_frame.png')\n",
    "        \n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[-1:],\n",
    "                              txt_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Last_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[-1:])[0]).save(f'{save_dir}/last_frame.png')\n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[0:1],\n",
    "                              txt_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/First_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[0:1])[0]).save(f'{save_dir}/first_frame.png')\n",
    "        \n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[5:6],\n",
    "                              txt_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Middle_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[5:6])[0]).save(f'{save_dir}/middle_frame.png')\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='L')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/Last_frame')\n",
    "\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='F')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/First_frame')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37050049",
   "metadata": {},
   "source": [
    "# Generate images for eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8c5ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fashion/fashion_train_videos.txt', 'r') as f:\n",
    "    train_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55f46200",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fashion/fashion_test_videos.txt', 'r') as f:\n",
    "     test_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c5926dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate(lst, save_dir, desc, bs=4):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(lst), bs)):\n",
    "            images, inversions = [], [],\n",
    "            for j in range(i, i+bs):\n",
    "                a, b, c, _ = load_video(lst[j])\n",
    "                images.append(a)\n",
    "                inversions.append(b)\n",
    "                sampleT = c\n",
    "            \n",
    "#             print(torch.stack(images, 0).shape, torch.stack(inversions, 0).shape)\n",
    "            edited_videos = forward_triple(model, torch.stack(images, 0), torch.stack(inversions, 0), sampleT, [desc] * bs)          \n",
    "            for video, video_name in zip(edited_videos, lst[i:i+bs]):\n",
    "                save_path = os.path.join(save_dir, video_name)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                for j, frame in enumerate(video):\n",
    "                    to_PIL(frame).save(os.path.join(save_path, f\"{j:06d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "163cc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3f823684",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c79c18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"A picture of a woman wearing a Women's Black Regular fit Blouse with a Round neckline and Long sleeves and a blue jean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6f708a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = np.array(train_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "201bd47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:35<00:00, 24.72s/it]\n"
     ]
    }
   ],
   "source": [
    "generate(test_videos[:200], test_save_dir, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a4573e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1024)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open('/kuacc/users/abond19/datasets/aligned_fashion_dataset/7c90376/00010.png').size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tokengan)",
   "language": "python",
   "name": "tokengan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
