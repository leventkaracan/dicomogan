{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73bd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea669fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/kuacc/users/mali18/dicomogan/logs/_510_face_attention2022-10-30T03-54-36'\n",
    "img_root = '/scratch/users/abond19/datasets/RAVDESS/Aligned'\n",
    "inverted_img_root = '/scratch/users/abond19/datasets/inverted_face_dataset'\n",
    "inversion_root = '/kuacc/users/abond19/datasets/w+_face_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e53366ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/kuacc/users/mali18/dicomogan/logs/tmp_log/38_k___541_fashion_selfattention2022-10-30T03-54-43/__541_fashion_selfattention2022-10-30T03-54-43'\n",
    "img_root = '/kuacc/users/abond19/datasets/aligned_fashion_dataset'\n",
    "inverted_img_root =  '/kuacc/users/abond19/datasets/inverted_fashion_dataset'\n",
    "inversion_root =  '/kuacc/users/abond19/datasets/w+_fashion_dataset/fashion/PTI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5250db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81fc09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from /kuacc/users/mali18/dicomogan/logs/_510_face_attention2022-10-30T03-54-36/VideoManipulation/_510_face_attention2022-10-30T03-54-36/checkpoints/epoch=116-step=32291.ckpt\n"
     ]
    }
   ],
   "source": [
    "from experiments_utils import *\n",
    "model = load_model_from_dir(model_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "89b6fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_start = 5\n",
    "n_frames = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1710ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "import os\n",
    "import torch \n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "IMG_EXTENSIONS = ['.png', '.PNG']\n",
    "TXT_EXTENSIONS = ['.txt']\n",
    "\n",
    "crop = None\n",
    "size = None\n",
    "trans_list = []\n",
    "if crop is not None:\n",
    "    trans_list.append(transforms.CenterCrop(tuple(crop)))\n",
    "if size is not None:\n",
    "    trans_list.append(transforms.Resize(tuple(size)))\n",
    "trans_list.append(transforms.ToTensor())\n",
    "img_transform=transforms.Compose(trans_list)\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def is_text_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in TXT_EXTENSIONS)\n",
    "\n",
    "def get_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return img\n",
    "\n",
    "def get_inversion(inversion_path):\n",
    "    w_vector = torch.load(inversion_path, map_location='cpu')\n",
    "    if len(w_vector.shape) == 2:\n",
    "        w_vector = w_vector.unsqueeze(0)\n",
    "    assert (w_vector.shape == (1, 18, 512)), \"Inverted vector has incorrect shape\"\n",
    "    return w_vector\n",
    "\n",
    "def load_video(vid_path):\n",
    "    images, inversions, sampleT, inversion_imgs = [], [], [], [] \n",
    "    fname = vid_path\n",
    "    \n",
    "    lst_vid = sorted(os.listdir(os.path.join(img_root, fname)))\n",
    "    def correct(f):\n",
    "        if is_image_file(f):\n",
    "            return int(f.split('.')[0])\n",
    "        else:\n",
    "            return f\n",
    "    lst_vid = list(map(correct, lst_vid))[:-1]\n",
    "#     print(lst_vid)\n",
    "    inds = np.argsort(lst_vid)\n",
    "#     print(inds)\n",
    "    inds = inds[n_start:n_start+n_frames]\n",
    "    for f in np.array(sorted(os.listdir(os.path.join(img_root, fname))))[inds]:\n",
    "        if is_image_file(f):\n",
    "#             print(f)\n",
    "            imname = f[:-4]\n",
    "            images.append(img_transform(get_image(os.path.join(img_root, fname, f))))\n",
    "            inversion_imgs.append(img_transform(get_image(os.path.join(inverted_img_root, fname, f))))\n",
    "            inversions.append(get_inversion(os.path.join(os.path.join(inversion_root, fname, imname + \".pt\"))))\n",
    "            sampleT.append(int(imname))\n",
    "    \n",
    "    with open(os.path.join(img_root, fname, 'updated_descriptions.txt')) as f:\n",
    "        description = f.readlines()[0]\n",
    "\n",
    "    return torch.stack(images).to(device), torch.cat(inversions, 0).to(device), torch.Tensor(sampleT).to(device), torch.stack(inversion_imgs).to(device), description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a17eeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torchvision\n",
    "def save_gif(video, range, save_path):\n",
    "    # Assuming that the current shape is T * B x C x H x W\n",
    "    with imageio.get_writer(save_path, mode='I') as writer:\n",
    "       for b_frames in video:\n",
    "            # b_frames B x C x H x W\n",
    "            frame = torchvision.utils.make_grid(b_frames,\n",
    "                            nrow=b_frames.shape[0],\n",
    "                            normalize=True,\n",
    "                            range=range).detach().cpu().numpy()\n",
    "            frame = (np.transpose(frame, (1, 2, 0)) * 255).astype(np.uint8)\n",
    "            writer.append_data(frame)\n",
    "\n",
    "#     wandb.log({name: wandb.Video(filename, fps=2, format=\"gif\")})\n",
    "#     os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "244fd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    '8c110571',\n",
    "    '2c111960',\n",
    "    '8c110147'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a9e3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    'Actor_03_114',\n",
    "    'Actor_04_97',\n",
    "    'Actor_05_99'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba594e",
   "metadata": {},
   "source": [
    "# Reconstruct Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ef84efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 14., 15., 16., 17., 18., 19.,  2., 20., 21.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sampleT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ccfc5dd3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.png\n",
      "1.png\n",
      "10.png\n",
      "11.png\n",
      "12.png\n",
      "13.png\n",
      "14.png\n",
      "15.png\n",
      "16.png\n",
      "17.png\n",
      "18.png\n",
      "19.png\n",
      "2.png\n",
      "20.png\n",
      "21.png\n",
      "22.png\n",
      "23.png\n",
      "24.png\n",
      "25.png\n",
      "26.png\n",
      "27.png\n",
      "28.png\n",
      "29.png\n",
      "3.png\n",
      "30.png\n",
      "31.png\n",
      "4.png\n",
      "5.png\n",
      "6.png\n",
      "7.png\n",
      "8.png\n",
      "9.png\n",
      "updated_descriptions.txt\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [00:10<00:21, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.png\n",
      "1.png\n",
      "10.png\n",
      "11.png\n",
      "12.png\n",
      "13.png\n",
      "14.png\n",
      "15.png\n",
      "16.png\n",
      "17.png\n",
      "18.png\n",
      "19.png\n",
      "2.png\n",
      "20.png\n",
      "21.png\n",
      "22.png\n",
      "23.png\n",
      "24.png\n",
      "25.png\n",
      "26.png\n",
      "27.png\n",
      "28.png\n",
      "29.png\n",
      "3.png\n",
      "30.png\n",
      "31.png\n",
      "32.png\n",
      "33.png\n",
      "34.png\n",
      "35.png\n",
      "36.png\n",
      "4.png\n",
      "5.png\n",
      "6.png\n",
      "7.png\n",
      "8.png\n",
      "9.png\n",
      "updated_descriptions.txt\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [00:20<00:10, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.png\n",
      "1.png\n",
      "10.png\n",
      "11.png\n",
      "12.png\n",
      "13.png\n",
      "14.png\n",
      "15.png\n",
      "16.png\n",
      "17.png\n",
      "18.png\n",
      "19.png\n",
      "2.png\n",
      "20.png\n",
      "21.png\n",
      "22.png\n",
      "23.png\n",
      "24.png\n",
      "25.png\n",
      "26.png\n",
      "27.png\n",
      "28.png\n",
      "29.png\n",
      "3.png\n",
      "30.png\n",
      "4.png\n",
      "5.png\n",
      "6.png\n",
      "7.png\n",
      "8.png\n",
      "9.png\n",
      "updated_descriptions.txt\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n",
      "torch.Size([5, 512]) torch.Size([1, 18, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:30<00:00, 10.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch.nn as nn\n",
    "with torch.no_grad():\n",
    "    for video in tqdm(videos):\n",
    "        images, inversions, sampleT, inversion_imgs = load_video(video)\n",
    "        save_dir = os.path.join('applications_results', f\"{model_dir.split('/')[-1]}\", video)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # org\n",
    "        save_gif(images, (0, 1), f'{save_dir}/original.gif')\n",
    "        \n",
    "        # inversion\n",
    "        save_gif(inversion_imgs, (0, 1), f'{save_dir}/inversion.gif')\n",
    "        \n",
    "        tgt_desc = \"A picture of a woman\"\n",
    "        txt_feat = model.clip_encode_text([tgt_desc])  # - model.clip_encode_text([src_desc])\n",
    "        \n",
    "        ind = np.random.randint(len(sampleT))\n",
    "        frame_feat = model.clip_loss.encode_images(images[ind:ind+1])\n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions.mean(0, keepdims=True),\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Mean_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions.mean(0, keepdims=True))[0]).save(f'{save_dir}/mean_frame.png')\n",
    "        \n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[-1:],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Last_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[-1:])[0]).save(f'{save_dir}/last_frame.png')\n",
    "        \n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[0:1],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/First_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[0:1])[0]).save(f'{save_dir}/first_frame.png')\n",
    "        \n",
    "        \n",
    "        mid = inversions.shape[0] // 2\n",
    "        edited_videos = model(images.unsqueeze(0), \n",
    "                              sampleT,\n",
    "                              inversions[mid:mid+1],\n",
    "                              txt_feat,\n",
    "                             frame_feat)[0]\n",
    "        save_gif(edited_videos, (-1, 1), f'{save_dir}/Middle_Frame.gif')\n",
    "        to_PIL(model.stylegan_G(inversions[mid:mid+1])[0]).save(f'{save_dir}/middle_frame.png')\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='L')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/Last_frame')\n",
    "\n",
    "#         edited_videos = forward(model, images.unsqueeze(0), inversions.unsqueeze(0), sampleT, [des], flag='F')[0]\n",
    "#         save_gif(edited_videos.permute(0, 2, 3, 1).detach().cpu().numpy(), f'{save_dir}/First_frame')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37050049",
   "metadata": {},
   "source": [
    "# Generate images for eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a59e5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = 'data/fashion/fashion_train_videos.txt'\n",
    "# test_path = 'data/fashion/fashion_test_videos.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "368d48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/kuacc/users/mali18/dicomogan/data/face/action_train.txt'\n",
    "test_path = '/kuacc/users/mali18/dicomogan/data/face/action_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a8c5ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, 'r') as f:\n",
    "    train_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55f46200",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_path, 'r') as f:\n",
    "     test_videos = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c5926dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate(lst, save_dir, bs=2, save_size=(256,256)):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(lst), bs)):\n",
    "            images, inversions, descriptions = [], [], []\n",
    "            for j in range(i, i+bs):\n",
    "                a, b, c, _, desc = load_video(lst[j])\n",
    "                images.append(a)\n",
    "                inversions.append(b)\n",
    "                sampleT = c\n",
    "                descriptions.append(desc)\n",
    "            \n",
    "            images, inversions = torch.stack(images, 0), torch.stack(inversions, 0)\n",
    "            txt_feat = model.clip_encode_text(descriptions)  # - model.clip_encode_text([src_desc])\n",
    "\n",
    "            ind = np.random.randint(len(sampleT))\n",
    "            frame_feat = model.clip_loss.encode_images(images[:, ind])\n",
    "\n",
    "#             print(images.shape, sampleT.shape, inversions.mean(1, keepdims=False).shape, txt_feat.shape, frame_feat.shape)\n",
    "            edited_videos = model(images, \n",
    "                                  sampleT,\n",
    "                                  inversions.mean(1, keepdims=False),\n",
    "                                  txt_feat,\n",
    "                                  frame_feat)\n",
    "\n",
    "    # #             print(torch.stack(images, 0).shape, torch.stack(inversions, 0).shape)\n",
    "    #             edited_videos = model(torch.stack(images, 0), torch.stack(inversions, 0), sampleT, [desc] * bs)          \n",
    "            for video, video_name in zip(edited_videos, lst[i:i+bs]):\n",
    "                save_path = os.path.join(save_dir, video_name)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                for j, frame in enumerate(video):\n",
    "                    to_PIL(frame).resize(save_size).save(os.path.join(save_path, f\"{j:06d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "163cc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3f823684",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save_dir = os.path.join(\"model_outputs\", f\"{model_dir.split('/')[-1]}\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6f708a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = np.array(train_videos)\n",
    "test_videos = np.array(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9daa6c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:14<00:00,  5.54s/it]\n"
     ]
    }
   ],
   "source": [
    "generate(train_videos[:200], train_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "201bd47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [08:54<00:52,  5.87s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "t must be strictly increasing or decreasing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_143236/3463102137.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_videos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_save_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_143236/4283397410.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(lst, save_dir, bs, save_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                   \u001b[0minversions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                   \u001b[0mtxt_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                   frame_feat)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# #             print(torch.stack(images, 0).shape, torch.stack(inversions, 0).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tokengan/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/mali18/dicomogan/dicomogan/models/tmp_dicomoclip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vid_bf, sampleT, mean_inversion, txt_feat, frame_feat, mask)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# vae encode frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mrep_video_dynamics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_dynamic_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_bf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mframe_dynamics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_frames_dynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_video_dynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# T * B x D x H' x W'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mframe_dynamics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_dynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_dynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# T * B x H' * W' x D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mframe_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_dynamics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# T*B x D1+D2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/mali18/dicomogan/dicomogan/models/tmp_dicomoclip.py\u001b[0m in \u001b[0;36msample_frames_dynamics\u001b[0;34m(self, video_dynamics, ts)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_frames_dynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_dynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbVAE_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_ode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_dynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# T * B x D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_inversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/mali18/dicomogan/dicomogan/new_modules.py\u001b[0m in \u001b[0;36msolve_ode\u001b[0;34m(self, zd0, t)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mzdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffeq_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzd0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x T x spatial_code_ch x H' x W'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mzdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# T * B x spatial_code_ch x H' x W'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tokengan/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/mali18/dicomogan/dicomogan/vidode.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, first_point, time_steps_to_predict, backwards)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         pred_y = odeint(self.ode_func, first_point, time_steps_to_predict,\n\u001b[0;32m---> 65\u001b[0;31m                         rtol=self.odeint_rtol, atol=self.odeint_atol, method=self.ode_method)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# => [b, t, c, h0, w0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tokengan/lib/python3.7/site-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_is_reversed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOLVERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSOLVERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tokengan/lib/python3.7/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36m_check_inputs\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn, SOLVERS)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Normalise time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0m_check_timelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0mt_is_reversed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tokengan/lib/python3.7/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36m_check_timelike\u001b[0;34m(name, timelike, can_grad)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtimelike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{} cannot require gradient\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimelike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtimelike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{} must be strictly increasing or decreasing'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: t must be strictly increasing or decreasing"
     ]
    }
   ],
   "source": [
    "generate(test_videos[:200], test_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a4573e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1024)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open('/kuacc/users/abond19/datasets/aligned_fashion_dataset/7c90376/00010.png').size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tokengan)",
   "language": "python",
   "name": "tokengan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
