{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d47f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "local_state = np.random.RandomState(10)\n",
    "local_state.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a92661",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dbf5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from experiments_utils import *\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "from dicomogan.modules import MultiscaleDiscriminatorPixpixHDMFMOD\n",
    "from dicomogan.modules import MappingNetworkVAE, Generator2, TextEncoder, EncoderVideo_LatentODE, Decoder,LatentODEfunc\n",
    "from dicomogan.losses.loss_lib import GANLoss, VGGLoss\n",
    "import random\n",
    "from torchdiffeq import odeint\n",
    "import clip\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11a37832",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4283557045.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9084/4283557045.py\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    def validation_step():\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class DiCoMOGAN(pl.LightningModule):\n",
    "    def __init__(self, ODE_func_config, \n",
    "                    video_ecnoder_config,\n",
    "                    video_decoder_config,\n",
    "                    text_encoder_config,\n",
    "                    discriminator_config,\n",
    "                    generator_config,\n",
    "                    mapping_config,\n",
    "                    ):\n",
    "        self.clip_img_transform = transforms.Compose([\n",
    "                    transforms.Resize(224, interpolation=Image.BICUBIC),\n",
    "                    transforms.CenterCrop(224), \n",
    "                    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n",
    "\n",
    "\n",
    "        # initialize model\n",
    "        self.func = instantiate_from_config(ODE_func_config) \n",
    "        self.bVAE_enc = instantiate_from_config(video_ecnoder_config, odefunc=self.func) \n",
    "        self.bVAE_dec = instantiate_from_config(video_decoder_config)\n",
    "        self.text_enc = instantiate_from_config(text_encoder_config)\n",
    "        self.D = instantiate_from_config(discriminator_config)\n",
    "        self.G = instantiate_from_config(generator_config)\n",
    "        mapping = instantiate_from_config(mapping_config) \n",
    "\n",
    "        clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "        clip_model.requires_grad_(False)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx=0):\n",
    "\n",
    "\n",
    "    def validation_step():\n",
    "\n",
    "    \n",
    "    def configure_opemizers():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_enc_optimizer = torch.optim.Adam(bVAE_enc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "vae_dec_optimizer = torch.optim.Adam(bVAE_dec.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "text_enc_optimizer = torch.optim.Adam(text_enc.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "m_optimizer = torch.optim.Adam(mapping.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "logf = open(\"./examples_3dshapes/log_max.txt\", 'w')\n",
    "vae_cond_dim = 3\n",
    "ode_dim = 1\n",
    "for epoch in range(epoch_no, args.num_epochs):\n",
    "\n",
    "    # training loop\n",
    "    avg_D_real_loss = 0\n",
    "    avg_Dt_real_loss = 0\n",
    "    avg_D_real_m_loss = 0\n",
    "    avg_D_real_m_vae_loss = 0\n",
    "    avg_D_real_m_both_loss = 0\n",
    "    avg_D_real_v_loss = 0\n",
    "    avg_Dt_real_m_loss = 0\n",
    "    avg_D_fake_loss = 0\n",
    "    avg_D_fake_vae_loss = 0\n",
    "    avg_D_fake_both_loss = 0\n",
    "    avg_Dt_fake_loss = 0\n",
    "    avg_G_fake_loss = 0\n",
    "    avg_G_fake_temp_loss = 0\n",
    "    avg_vgg_loss = 0\n",
    "    avg_vae_loss = 0\n",
    "    avg_embedvae_loss = 0\n",
    "    avg_temp_vggflow_loss = 0\n",
    "    avg_attention_loss = 0\n",
    "    avg_unsup_loss = 0\n",
    "    avg_D_real_a_loss = 0\n",
    "    avg_ganvae_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        vid = batch['img'].cuda()\n",
    "        input_desc = batch['raw_desc']\n",
    "        \n",
    "        sampleT = batch['sampleT']\n",
    "        \n",
    "        assert torch.all(sampleT[0] == sampleT[np.randint(sampleT.size(0))]\n",
    "        sampleT = sampleT[0] # all batch['sampleT'] are the same\n",
    "\n",
    "        text = clip.tokenize([*input_desc[0][0]]).to(self.device)\n",
    "        txt_feat = self.clip_model.encode_text(text).float()\n",
    "\n",
    "                         \n",
    "        bs, T, ch, height, width = vid.size()\n",
    "        ts = (sampleT)*0.01\n",
    "        ts = ts - ts[0] # Question: if the first frame is not zero do we subtract?\n",
    "\n",
    "        vid_norm = vid * 2 - 1\n",
    "        txt_feat = txt_feat.unsqueeze(0).repeat(4,1,1)\n",
    "        txt_feat = txt_feat.view(bs * 4, -1)\n",
    "\n",
    "\n",
    "        requires_grad(bVAE_enc, True)\n",
    "        requires_grad(text_enc, True)\n",
    "        requires_grad(bVAE_dec, True)\n",
    "\n",
    "        requires_grad(G, False)\n",
    "        requires_grad(D, False)\n",
    "        requires_grad(mapping, False)\t\n",
    "\n",
    "        bVAE_enc.zero_grad()\n",
    "        text_enc.zero_grad()\n",
    "        bVAE_dec.zero_grad()\n",
    "\n",
    "        zs, zd, mu_logvar_s, mu_logvar_d = bVAE_enc(vid[:, sampleT], ts)\n",
    "        z_vid = torch.cat((zs, zd), 1)\n",
    "\n",
    "        #print(z_vid.size())\n",
    "\n",
    "        total_vae_loss = 0.0\n",
    "        beta_vae_loss = 0.0\n",
    "        recon_loss = 0.0\n",
    "        kl_loss = 0.0\n",
    "\n",
    "\n",
    "        muT, logvarT = text_enc(txt_feat)\n",
    "        zT = reparametrize(muT, logvarT)\n",
    "\n",
    "        #print(zT.size())\n",
    "        x_reconT = bVAE_dec(torch.cat((zT,z_vid[:, 3:]), 1))\n",
    "\n",
    "        x_recon = bVAE_dec(z_vid)\n",
    "        video_sample = vid[:, sampleT[:]]\n",
    "        video_sample = video_sample.permute(1,0,2,3,4)\n",
    "        video_sample = video_sample.contiguous().view(bs * 4, ch, height, width)\n",
    "\n",
    "        recon_loss = reconstruction_loss(video_sample, x_recon, 'bernoulli')\n",
    "        recon_lossT = reconstruction_loss(video_sample, x_reconT, 'bernoulli')\n",
    "\n",
    "        kl_loss_d = args.beta * kl_divergence(*mu_logvar_d)\n",
    "        kl_loss_s = args.beta * kl_divergence(*mu_logvar_s)\n",
    "        kl_loss = kl_loss_s +  kl_loss_d\n",
    "\n",
    "        beta_vae_loss = 0.5 * (recon_loss + recon_lossT) +  kl_loss\n",
    "        beta_vae_loss.backward()\n",
    "        total_vae_loss += beta_vae_loss\n",
    "        avg_vae_loss += total_vae_loss.data.item()\n",
    "\n",
    "        n_train_steps += 1\n",
    "\n",
    "        vae_enc_optimizer.step()\n",
    "        vae_dec_optimizer.step()\n",
    "        text_enc_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        vid_vis = video_sample.reshape(4, bs, ch, height, width)\n",
    "        rec_vis = x_recon.reshape( 4, bs, ch, height, width)\n",
    "        save_image((vid_vis[:, 0].data), './examples/real/epoch_%d.png' % (epoch + 1))\n",
    "        save_image((rec_vis[:, 0].data), './examples/recon/epoch_%d.png' % (epoch + 1))\n",
    "\n",
    "        # UPDATE DISCRIMINATOR\n",
    "        requires_grad(bVAE_enc, False)\n",
    "        requires_grad(text_enc, False)\n",
    "        requires_grad(bVAE_dec, False)\n",
    "\n",
    "        requires_grad(G, False)\n",
    "        requires_grad(D, True)\n",
    "        requires_grad(mapping, False)\n",
    "\n",
    "        D.zero_grad()\n",
    "\n",
    "\n",
    "        zs, zd, mu_logvar_s, mu_logvar_d = bVAE_enc(vid[:, sampleT], ts)\n",
    "        z_vid = torch.cat((zs, zd), 1)\n",
    "\n",
    "        video_sample = vid_norm[:, sampleT[:]]\n",
    "        video_sample = video_sample.permute(1,0,2,3,4)\n",
    "        video_sample = video_sample.contiguous().view(bs * 4, ch, height, width)\n",
    "\n",
    "\n",
    "        # real image with real latent)\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        real_logit = D(video_sample, txt_feat, latentw)\n",
    "        real_loss = criterionGAN(real_logit, True)\n",
    "        avg_D_real_loss += real_loss.data.item()\n",
    "        real_loss.backward(retain_graph=True)\n",
    "\n",
    "        # real image with mismatching text\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        txt_feat_mismatch,_ = preprocess_feat(txt_feat)\n",
    "        real_m_logit = D(video_sample, txt_feat_mismatch, latentw)\n",
    "        real_m_loss = 0.5/3 * criterionGAN(real_m_logit,False)\n",
    "        avg_D_real_m_loss += real_m_loss.data.item()\n",
    "        real_m_loss.backward(retain_graph=True)\n",
    "\n",
    "        # real image with mismatching vae\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        latentw_mismatch,_ = preprocess_feat(latentw)\n",
    "        real_m_logit = D(video_sample, txt_feat, latentw_mismatch)\n",
    "        real_m_loss = 0.5/3 * criterionGAN(real_m_logit,False)\n",
    "        avg_D_real_m_vae_loss += real_m_loss.data.item()\n",
    "        real_m_loss.backward(retain_graph=True)\n",
    "\n",
    "        # real image with mismatching text and vae\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        latentw_mismatch,_ = preprocess_feat(latentw)\n",
    "        txt_feat_mismatch,_ = preprocess_feat(txt_feat)\n",
    "        real_m_logit = D(video_sample, txt_feat_mismatch, latentw_mismatch)\n",
    "        real_m_loss = 0.5/3 * criterionGAN(real_m_logit,False)\n",
    "        avg_D_real_m_both_loss += real_m_loss.data.item()\n",
    "        real_m_loss.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "        # synthesized image with semantically relevant text\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        _,txt_feat_relevant = preprocess_feat(txt_feat)\n",
    "        fake = G(video_sample, txt_feat_relevant, latentw)\n",
    "        fake_logit = D(fake.detach(), txt_feat_relevant, latentw)\n",
    "        fake_loss =  0.5/3 * criterionGAN(fake_logit, False)\n",
    "        avg_D_fake_loss += fake_loss.data.item()\n",
    "        fake_loss.backward(retain_graph=True)\n",
    "\n",
    "        # synthesized image with semantically relevant vae\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        _,latentw_relevant = preprocess_feat(latentw)\n",
    "        fake = G(video_sample, txt_feat, latentw_relevant)\n",
    "        fake_logit = D(fake.detach(), txt_feat, latentw_relevant)\n",
    "        fake_loss =  0.5/3 * criterionGAN(fake_logit, False)\n",
    "        avg_D_fake_vae_loss += fake_loss.data.item()\n",
    "        fake_loss.backward(retain_graph=True)\n",
    "\n",
    "        # synthesized image with semantically relevant text and vae \n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        _,latentw_relevant = preprocess_feat(latentw)\n",
    "        _,txt_feat_relevant = preprocess_feat(txt_feat)\n",
    "        fake = G(video_sample, txt_feat_relevant, latentw_relevant)\n",
    "        fake_logit = D(fake.detach(), txt_feat_relevant, latentw_relevant)\n",
    "        fake_loss =  0.5/3 * criterionGAN(fake_logit, False)\n",
    "        avg_D_fake_both_loss += fake_loss.data.item()\n",
    "        fake_loss.backward(retain_graph=True)\n",
    "\n",
    "        d_optimizer.step()\n",
    "\n",
    "        unsup_weight = min(1, ((1 - 1e-5) / 5000) * n_train_steps + 1e-5)\n",
    "        #unsup_weight = 1\n",
    "        for g in vae_enc_optimizer.param_groups:\n",
    "            g['lr'] = vae_enc_lr * unsup_weight\n",
    "\n",
    "        # UPDATE GENERATOR\n",
    "\n",
    "        requires_grad(bVAE_enc, True)\n",
    "        requires_grad(bVAE_dec, True)\n",
    "        requires_grad(text_enc, False)\n",
    "\n",
    "        requires_grad(mapping, True)\n",
    "        requires_grad(G, True)\n",
    "        requires_grad(D, False)\n",
    "\n",
    "        G.zero_grad()\n",
    "        mapping.zero_grad()\n",
    "        bVAE_enc.zero_grad()\n",
    "        bVAE_dec.zero_grad()\n",
    "\n",
    "        zs, zd, mu_logvar_s, mu_logvar_d = bVAE_enc(vid[:, sampleT], ts)\n",
    "        z_vid = torch.cat((zs, zd), 1)\n",
    "\n",
    "        zsplits = torch.split(z_vid, int((bs * 4)/2), 0)\n",
    "        z_rel = torch.cat((torch.roll(zsplits[0], -1, 0), zsplits[1]), 0)\n",
    "\n",
    "        imgsplits = torch.split(video_sample, int((bs * 4)/2), 0)\n",
    "        img_rel = torch.cat((torch.roll(imgsplits[0], -1, 0), imgsplits[1]), 0)\n",
    "\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        _,txt_feat_relevant = preprocess_feat(txt_feat)\n",
    "        fake1 = G(video_sample, txt_feat_relevant, latentw)\n",
    "        fake_logit = D(fake1,txt_feat_relevant, latentw)\n",
    "        fake_loss1 = 1.0/3 * criterionGAN(fake_logit, True)\n",
    "\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        _,latentw_relevant = preprocess_feat(latentw)\n",
    "        fake2 = G(video_sample, txt_feat, latentw_relevant)\n",
    "        fake_logit = D(fake2, txt_feat, latentw_relevant)\n",
    "        fake_loss2 = 1.0/3 * criterionGAN(fake_logit, True)\n",
    "\n",
    "        latentw = mapping(z_vid[:,vae_cond_dim:])\n",
    "        _,latentw_relevant = preprocess_feat(latentw)\n",
    "        _,txt_feat_relevant = preprocess_feat(txt_feat)\n",
    "        fake3 = G(video_sample, txt_feat_relevant, latentw_relevant)\n",
    "        fake_logit = D(fake3, txt_feat_relevant, latentw_relevant)\n",
    "        fake_loss3 = 1.0/3 * criterionGAN(fake_logit, True)\n",
    "\n",
    "        #vgg_loss =  (criterionVGG(fake3, img_rel) + criterionVGG(fake2, img_rel) + criterionVGG(fake1, img_rel))*(1.0/3.0)\n",
    "        vgg_loss =  (criterionVGG(fake3, video_sample) + criterionVGG(fake2, video_sample) + criterionVGG(fake1, video_sample))*(1.0/3.0)\n",
    "\n",
    "        avg_G_fake_loss += (fake_loss1 + fake_loss2 + fake_loss3).data.item()\n",
    "        avg_vgg_loss += vgg_loss.data.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fake_sample_rs = fake1.view(4, bs, ch, height, width)\n",
    "        fake_sample_rs = fake_sample_rs.permute(1,0,2,3,4)\n",
    "        fake_sample = fake.view(4, bs, ch, height, width)\n",
    "        fake_sample = fake_sample.permute(1,0,2,3,4)\n",
    "        zs_vid_fake, zd_vid_fake, mu_logvar_fake_s, mu_logvar_fake_d = bVAE_enc((fake_sample_rs+1)*0.5, ts)\n",
    "        z_vid_fake = torch.cat((zs_vid_fake, zd_vid_fake), 1)\n",
    "\n",
    "        zsplits = torch.split(z_vid, int((bs * 4)/2), 0)\n",
    "        z_rel = torch.cat((torch.roll(zsplits[0], -1, 0), zsplits[1]), 0)\n",
    "        unsup_loss = criterionUnsupFactor(z_vid_fake, z_rel)*(1.0/3.0)\n",
    "\n",
    "\n",
    "        fake_sample_rs = fake2.view(4, bs, ch, height, width)\n",
    "        fake_sample_rs = fake_sample_rs.permute(1,0,2,3,4)\n",
    "        fake_sample = fake.view(4, bs, ch, height, width)\n",
    "        fake_sample = fake_sample.permute(1,0,2,3,4)\n",
    "        zs_vid_fake, zd_vid_fake, mu_logvar_fake_s, mu_logvar_fake_d = bVAE_enc((fake_sample_rs+1)*0.5, ts)\n",
    "        z_vid_fake = torch.cat((zs_vid_fake, zd_vid_fake), 1)\n",
    "        zsplits = torch.split(z_vid, int((bs * 4)/2), 0)\n",
    "        z_rel = torch.cat((torch.roll(zsplits[0], -1, 0), zsplits[1]), 0)\n",
    "        unsup_loss += criterionUnsupFactor(z_vid_fake, z_rel)*(1.0/3.0)\n",
    "\n",
    "        fake_sample_rs = fake3.view(4, bs, ch, height, width)\n",
    "        fake_sample_rs = fake_sample_rs.permute(1,0,2,3,4)\n",
    "        fake_sample = fake.view(4, bs, ch, height, width)\n",
    "        fake_sample = fake_sample.permute(1,0,2,3,4)\n",
    "        zs_vid_fake, zd_vid_fake, mu_logvar_fake_s, mu_logvar_fake_d = bVAE_enc((fake_sample_rs+1)*0.5, ts)\n",
    "        z_vid_fake = torch.cat((zs_vid_fake, zd_vid_fake), 1)\n",
    "        zsplits = torch.split(z_vid, int((bs * 4)/2), 0)\n",
    "        z_rel = torch.cat((torch.roll(zsplits[0], -1, 0), zsplits[1]), 0)\n",
    "        unsup_loss += criterionUnsupFactor(z_vid_fake, z_rel)*(1.0/3.0)\n",
    "        avg_unsup_loss += 0.5 * unsup_loss.data.item()\n",
    "\n",
    "        G_loss = fake_loss1 + fake_loss2 + fake_loss3 + 1.0 * vgg_loss + 0.5 * unsup_loss\n",
    "        G_loss.backward()\n",
    "\n",
    "        g_optimizer.step()\n",
    "        m_optimizer.step()\n",
    "        vae_enc_optimizer.step()\n",
    "\n",
    "        for g in vae_enc_optimizer.param_groups:\n",
    "            g['lr'] = vae_enc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ca7643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "conf = OmegaConf.load('configs/base_newmodel.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1354c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "train_data = instantiate_from_config(conf.data.params.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a84cb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(train_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e914a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8682ee86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3, 14, 17, 18],\n",
       "        [ 3, 14, 17, 18],\n",
       "        [ 3, 14, 17, 18],\n",
       "        [ 3, 14, 17, 18],\n",
       "        [ 0,  1,  9, 14],\n",
       "        [ 0,  1,  9, 14],\n",
       "        [ 0,  1,  9, 14],\n",
       "        [ 0,  1,  9, 14]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['sampleT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05a43b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 1, 18, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['inversion'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67374d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multi Shirts Made from cotton Regular fit Front button closure Spread collar Short sleeves Patch pocket in front All over ikat print Cropped length '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['raw_desc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e0b8c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blouses Green Floral embroidery detail Regular fit Cold shoulder style Square neckline Made from cotton Gathered front Short sleeves with tie-up '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['raw_desc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f10b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate_from_config(conf.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad79f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25766188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7230, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_step(batch, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc901d92",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiCoMOGAN' object has no attribute 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27357/281973652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/users/mali18/dicomogan/dicomogan/models/dicomogan.py\u001b[0m in \u001b[0;36mconfigure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                  \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbVAE_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tokengan/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DiCoMOGAN' object has no attribute 'learning_rate'"
     ]
    }
   ],
   "source": [
    "model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9182933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33f1cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "lin1 = nn.Linear(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbcbdbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(500, 10)\n",
    "res_a = torch.relu(lin1(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "946ae16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0195, 0.0000, 0.0000, 0.0000], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e15806a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0195, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.relu(lin1(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18bec09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.relu(lin1(a[0])) == res_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2c963da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9802e-08, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.relu(lin1(a[0])) - res_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827cf6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tokengan)",
   "language": "python",
   "name": "tokengan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
